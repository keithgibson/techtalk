---
title: Neural Networks and Transformers
revealOptions:
  transition: 'slide'
---

<!-- .slide: data-background-image="assets/sponsorSlide1start_dark_highres.png" -->

–––––––––––––––––––––––––––––––––––––––––––––––

#### Neural Networks and Transformers

_An introduction_

<div style="position: fixed; width: 100%; font-size: 0.4em">
<p>by Keith Gibson</p>
</div>

Note:
Welcome to this tech talk on neural networks and transformers.

Today, I'm hoping to provide a baseline understanding of these concepts. As such, I'll be erring on the side of simplification.

We'll start with neural networks broadly, followed by the key characteristics of the transformer, the fundamental neural network architecture of today's most advanced language models, including the GPT series and the chatbot ChatGPT.

–––––––––––––––––––––––––––––––––––––––––––––––

So...what is a Neural Network?

Note:
Well, let's start by being more precise and instead ask -

# down

----

What's an *Artificial* Neural Network?

Note:
What's an *Artificial* Neural Network?

I think more precision is helpful because an 'artificial' neural network is inspired by and roughly designed after another kind of neural network. That is, the biological one, in the brain.

## EDIT: say more here about the overlap between the biological and artificial neural networks, or find a better transition.

Both are networks of interconnected - you guessed it - neurons
# down

----

Note:
/take a beat

But on its own

# down


–––––––––––––––––––––––––––––––––––––––––––––––
<!-- .slide: data-background-color="#2c2c2c"-->
<!-- .slide: data-preload data-background-interactive data-background-iframe="interactive.html"-->

<table class="fragment" data-fragment-index="1">
  <tr>
    <th class="fragment" data-fragment-index="1">Biological</th>
    <th class="fragment" data-fragment-index="3">Artificial</th>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="2">synapse</td>
    <td class="fragment" data-fragment-index="4">dimension</td>
  </tr>
  <tr>
    <td class="fragment" data-fragment-index="5">signal</td>
    <td class="fragment" data-fragment-index="6">value</td>
  </tr>
</table>

Note:
in a vacuum, what value does one neuron have? What can it accomplish?

Well, none, nothing.

It is only in _relation_

# click

to other neurons,

only when _connected_

# click

to other neurons,

that it can take on value, that it can accomplish something.

Maybe a network of neurons in relationship with one another

In biological networks, this connection between neurons is called a synapse. It takes the form of a chemical or electrical signal.
In artificial networks, we can liken this connection to what is called a dimension. Which is to say, a number. The value of the number is equivalent to the synapse's signal.


# click

help: fact check and clarify the above


–––––––––––––––––––––––––––––––––––––––––––––––

And how do they work, roughly?

----
1. Input received, converted

  B: sensory input => chemical or electrical signal  <!-- .element: class="fragment fade-up" data-fragment-index="1" -->

  A: text => tokens => dimensions <!-- .element: class="fragment fade-up" data-fragment-index="2" -->

----
2. Input processed through successive layers

<span class="fragment fade-up-then-out" data-fragment-index="1">A: layer<sup>n output</sup> => layer<sup>n+1 output</sup>  </span>

<span class="fragment fade-up-then-semi-out" data-fragment-index="2"> layer<sup>n+1 output</sup> => layer<sup>n+2 input</sup>  </span>

<span class="fragment fade-up-then-semi-out" data-fragment-index="3"> layer<sup>n+2 output</sup> => layer<sup>n+3 input</sup>  </span>

<span class="fragment fade-up" data-fragment-index="4"> etc. </span>

----
3. Output produced

B: conscious thought / involuntary action<!-- .element: class="fragment fade-up" data-fragment-index="1" -->

A: token<!-- .element: class="fragment fade-up" data-fragment-index="2" -->

Note:
In the biological network,
1.
Input is received and converted into a form that can be processed
  - sensory input is converted to a chemical or electrical signal.
  - text is converted to dimensions

2.
That input is processed through successive layers of neurons.
Each layer's output is the next layer's input.

help: biological examples between the initial touch and the conscious thought of being touchéd. would the subconscious layer be involved here or is that too slow for this sort of processing?

  The final intermediary layer might be the brain's subconscious processes, working behind the scenes to interpret and analyze information.

3.
The output is produced.
  Conscious thoughts or involuntary actions.
  Artificial: a token is predicted

–––––––––––––––––––––––––––––––––––––––––––––––

Okay. What's a Transformer?

Note: a transformer is a neural network, with the general architecture I just described. The difference between transformers and other text-centric neural networks is in how they process information in the intermediate layers.

–––––––––––––––––––––––––––––––––––––––––––––––
Chatbot example

Note:
let's walk through each step of the transformer model at a macro level, using the example of a chatbot powered by GPT-2, which was released in 2018.

----

"The quick brown fox"

Note:


----

input layer:

the input text is converted into numbers <!-- .element: class="fragment" data-fragment-index="1" -->

Note:

----

intermediate layer n<sup>0</sup>:

the numbers from the input layer are processed<!-- .element: class="fragment" data-fragment-index="1" -->

and fed to the subsequent layer, n<sup>1</sup> <!-- .element: class="fragment" data-fragment-index="2" -->


Note:
test

----

intermediate layer n<sup>1</sup>:

<span class="fragment" data-fragment-index="1"> the numbers from the prior intermediate layer are processed </span>

<span class="fragment" data-fragment-index="2"> and fed to the subsequent layer, n<sup>2</sup> </span>

Note:
test

----

and so on

Note:

----

output layer:

the numbers from the final intermediate layer are used to predict a word.<!-- .element: class="fragment" data-fragment-index="1" -->

Note:


We get an output of 1 word. In this case,

----


"jumps"

Note:
jumps.

----

<span class="fragment fade-in-then-semi-out" data-fragment-index="0"> The initial input  </span>
<span class="fragment fade-in-then-semi-out" data-fragment-index="2"> and the output  </span>

<span class="fragment" data-fragment-index="1"> "The quick brown fox"  </span>
<span class="fragment" data-fragment-index="3"> + "jumps"  </span>

<span class="fragment" data-fragment-index="4"> are fed back in.  </span>
Note:
Let's start high level.
"The quick brown fox".
----

repeat

----

"the quick brown fox"
<span class="fragment" data-fragment-index="1"> + </span>
<span class="fragment" data-fragment-index="2">"jumps</span>
<span class="fragment" data-fragment-index="3">over</span>
<span class="fragment" data-fragment-index="4">the</span>
<span class="fragment" data-fragment-index="5">lazy</span>
<span class="fragment" data-fragment-index="6">dog"</span>

Note:
Let's start high level.
"The quick brown fox".

input layer:
Each word gets converted to a series of numbers.


output layer:
   - The transformer predicts the most likely next word based on the processed input. Let's say it predicts "jumps".
   ----
   - The predicted word is added to the sequence: "The quick brown fox jumps".
   ----
   - The process repeats, with each new word influencing subsequent predictions.


–––––––––––––––––––––––––––––––––––––––––––––––

## GPT-2
```py [1|1,3|1,4|1,5|1,6|1,7|1,3-7]
# The model's own (hyper)parameters
    return HParams(
        n_vocab=0,
        n_ctx=1024,
        n_embd=768,
        n_head=12,
        n_layer=12,
    )
```
<br>
<div style="position: fixed; width: 100%; font-size: 0.4em">
  <a href="https://github.com/openai/gpt-2/blob/master/src/model.py">GPT-2 on Github</a>
</div>

Note:
  n_vocab=0
    this is the number of unique words in the dataset
    and determines the size of the input embeddings
    the number of tokens in the input sequence

  n_ctx=1024
    the maximum number of tokens in the context window
    determines the number of positions in the positional encoding

  n_embd=768
    the number of elements per embedding

  n_head=12
    the number of attention heads in each layer
    this is the number of times the model will split the input sequence
    and calculate attention weights

  n_layer=12
    the number of transformer layers
    this is the number of times the model will apply the transformer
    to the input sequence


–––––––––––––––––––––––––––––––––––––––––––––––

Let's go through that step by step.

–––––––––––––––––––––––––––––––––––––––––––––––

"The quick brown fox"


"quick": [ 3.249, 0.713, 1.687 ] <!-- .element: class="fragment fade-up" data-fragment-index="1" -->

"brown": [ 0.147, 2.198, -0.589 ] <!-- .element: class="fragment fade-up" data-fragment-index="2" -->

"fox": [ 4.875, -1.434, 2.302 ] <!-- .element: class="fragment fade-up" data-fragment-index="3" -->

Note:
INSERT THE array values in this slide
   - The input is split into tokens: "The", "quick", "brown", "fox".
   ----
   - Each token is mapped to its corresponding embedding vector, which is just a fancy way of saying an array of numbers.

  that leaves us with something like this:

–––––––––––––––––––––––––––––––––––––––––––––––

{
  [ -1.228, 0.733, -0.316 ],

  [ 3.249, 0.713, 1.687 ],

  [ 0.147, 2.198, -0.589 ],

  [ 4.875, -1.434, 2.302 ]
}

Note:
combine this slide with the slide below.
slide in the selfAttentionMechanism() function from the left if possible


–––––––––––––––––––––––––––––––––––––––––––––––


selfAttentionMechanism([ -1.228, 0.733, -0.316 ])

selfAttentionMechanism([ 3.249, 0.713, 1.687 ])<!-- .element: class="fragment fade-up" data-fragment-index="1" -->

selfAttentionMechanism([ 0.147, 2.198, -0.589 ])<!-- .element: class="fragment fade-up" data-fragment-index="2" -->

selfAttentionMechanism([ 4.875, -1.434, 2.302 ])<!-- .element: class="fragment fade-up" data-fragment-index="3" -->

note:
   - Each element in one array is evaluated against each element in every other array.

–––––––––––––––––––––––––––––––––––––––––––––––

[

  [0.13, 0.64, 0.82],

  [0.75, 0.39, 0.11],

  [0.95, 0.58, 0.26],

  [0.49, 0.92, 0.61]

]

–––––––––––––––––––––––––––––––––––––––––––––––

repeat for each intermediary layer

–––––––––––––––––––––––––––––––––––––––––––––––

predictionAlgorithm(lastIntermediateLayerOutputArray)

"jumps" <!-- .element: class="fragment" data-fragment-index="1" -->

Note:

–––––––––––––––––––––––––––––––––––––––––––––––

repeat

–––––––––––––––––––––––––––––––––––––––––––––––

Text Generation Step 2

2. Self-Attention and Multi-Head Attention:
   - The embeddings pass through the self-attention mechanism.
   ----

   - Multiple attention heads analyze different aspects simultaneously.

–––––––––––––––––––––––––––––––––––––––––––––––
Text Generation Step 3

3. Feedforward Networks and Positional Encoding:
   - The output from the attention mechanisms is refined by feedforward networks.
   ----
   - Positional information is added to preserve the order of the tokens.

–––––––––––––––––––––––––––––––––––––––––––––––
Text Generation Step 4

4. Text Generation:


Tokenization and Embeddings

- Input text is broken down into tokens
----
- Each token is mapped to an embedding vector

–––––––––––––––––––––––––––––––––––––––––––––––
Self-Attention Mechanism (SAM)

- Core of transformer's parallel processing
----
- Each word is processed in context of every other word
----
- Like a conversation where you recall every word spoken

–––––––––––––––––––––––––––––––––––––––––––––––
Multi-Head Self-Attention

- Multiple attention "heads" per layer
----
- Each head focuses on different aspects of input
----
- Heads work in parallel to analyze input from different perspectives

–––––––––––––––––––––––––––––––––––––––––––––––
Feedforward Networks (FFNs)

- Additional layers of processing
----
- Each node's output from self-attention is independently transformed
----
- Similar to how brain regions refine sensory input

–––––––––––––––––––––––––––––––––––––––––––––––
Positional Encoding

- Adds information about word position
----
- Ensures model understands word order
----
- Crucial for meaning

–––––––––––––––––––––––––––––––––––––––––––––––

<!-- .slide: data-background-image="assets/sponsorSlide2end_dark_highres.png" -->

–––––––––––––––––––––––––––––––––––––––––––––––

Works Cited

–––––––––––––––––––––––––––––––––––––––––––––––

github: @keithgibson
linkedin: @keithrgibson


–––––––––––––––––––––––––––––––––––––––––––––––

the floating point array

"The":<!-- .element: class="fragment fade-up-then-out" data-fragment-index="0" -->
<div style="font-size: 0.1em">
  [ -1.228084989880871, 0.7331303424681268, -0.31621513108079213, 3.1979479873077885,
  2.168531500398558, -0.24906143658303126, -1.5775041584541172, -2.5750147547274893,
  1.8264231424899657, 4.448246216443945, 4.616339993884374, -1.3465269175726058,
  3.7748507178965482, -3.8642399003090744, 4.062521558695886, -2.517490635540278,
  -2.3333924086374713, 3.5648055400233147, -2.6021759100785458, -1.135771848261835,
  -2.9654474015943855, -4.869958955438847, -2.650808042923687, 3.4223173974162684,
  -3.2136943656022066, 3.7986100981683144, -0.744268146989775, -4.330353006224592,
  -0.11111378230988844, 3.313439752993496, 2.228278000808473, 4.9385423385824865,
  -1.000830973374407, -4.050927756937563, -1.3952706641265378, -2.8959900801266025,
  -4.1606265450539155, -3.3955732427902796, -2.800161814783142, 3.7665116096857894,
  1.1209109637733583, -2.565782952316793, -3.9523509507958465, 2.004507294942826,
  -3.8758543633225995, 2.558028022387142, 2.900713793806733, 1.4453311927773882,
  -3.491279734921988, -2.0888390713429605, 2.068676766583099, -1.0860972373932043,
  -0.588063447701634, 2.6480768203751115, 2.1802425613048566, -3.534460879770114,
  1.9444991814782338, -3.469153999118253, -2.9688231412686794, 3.5144672413193128,
  -1.4507690777164939, 1.6708387894703343, -2.828568483001377, 2.927868774366531,
  1.882428290891001, -3.2226844516964226, 0.603798712731896, 0.6768682009967915,
  -1.0538763034175602, 3.317085656965224, -3.6496036337093107, -4.832416837142426,
  -4.104887022666206, -4.342720497152839, -0.2933567842991991, 4.677994993680381,
  4.6764450894814615, 0.12852253036618144, -2.848449629207639, 0.9330703492115244,
  2.8513162212161287, -1.749561968414981, 0.4042781217372564, 4.546375484426687,
  -4.887549295771008, 2.2757700679690256, 4.166888380692251, -1.213638558983372,
  -2.400714416130938, 2.7333341893584233, 3.0593041010927813, -0.8024652262126271,
  -2.4045718864962096, 1.472637116504849, -1.0788720676734376, -3.6910761878087928,
  0.8469579382851293, -1.42854340721829, -2.8302002265829906, -1.3524363118904015,
  3.692882900992032, -2.178818398114224, 3.139465827459194, -2.647686431328422, -1.094284580997582,
  4.034392500825906, 4.947139593394139, 4.5807777934306095, -4.826919960045261, 4.236861604078072,
  -1.6210904782827096, 3.1924419631666705, 3.762414936953091, -4.414964620615461,
  0.7532209059618182, 0.7363677742940222, -1.6814692812158238, 1.8441014415371182,
  1.8773451997356805, 2.038437924744203, -3.767566552039072, 1.8443596576364545,
  -4.928776391728949, -4.144488894325107, 0.9336799561166504, 3.8679556596084943,
  3.833924041933061, 4.785356501275167, -1.96990688700182, -3.1154054068984904, 4.456052257445746,
  -4.186934423684181, 1.5254035676343358, -2.7412336596313502, 1.412918842180808,
  -3.1543639917895794, -3.196521227960585, -1.0918743671116449, -3.4741635007855853,
  -4.313954922476794, 0.3608986718356473, 3.1992546156801343, 1.2287649870168753,
  2.964202786977486, -2.5268728479353686, -2.8002955909630534, 2.837741149804838,
  4.4839024643326475, 0.1738724025153644, 4.562796961768996, 0.9712754580075806,
  0.924708823440243, -1.7496831888966113, 3.283907252050909, -0.41372543992190103,
  3.5560196379284683, -0.02536080206953617, 0.2448488868963583, -0.25773107779995463,
  1.841221228252465, 2.3540036847241534, 2.2976334844201123, -1.6725178634072613,
  0.4596333402706998, -4.204776535311748, -1.2177381969546985, -4.830091648835209,
  -3.511337923868718, 2.23625613132842, -1.2437188669905241, 3.576509964655539, 4.895768495964074,
  -0.8438047173338781, 4.665699453390268, -3.7928730065979788, -2.60942717714836,
  4.760503592181278, 0.7625417011458762, -1.39045332448003, 4.036897610543765, 4.4933358217607395,
  -0.9327465470303657, -2.063979133064582, -2.2690184094344024, 1.1632365874384636,
  3.761675620376767, 2.550743505259221, 2.0420808590803974, 4.853035850512121, -4.806687475486298,
  -1.9321707343794614, -0.04455661109342035, -3.0209386209879208, -3.624052483150244,
  -2.91928035751551, -0.5152340014675332, 0.13230534693042362, -4.915669022509781,
  -0.6373811564637988, -2.3575434920373994, -4.439965326502473, -2.704691331872906,
  3.9430079627397614, 2.5236166951211425, -4.3127426973259775, -2.5566960146759476,
  0.9874793436746181, 0.3006221376130602, -0.234130150852911, -0.05569158527966955,
  -2.0525523279689395, 4.791885387592378, -1.9578492817313875, 2.5203424309457905,
  -0.1767492754566664, -0.5451169145965595, 4.429659806556938, -4.636582685047658,
  -4.843766708725568, -1.723500074248745, 3.653264705082808, -1.5528585466064015,
  -2.329942277226993, -3.574377037081542, -3.5895272891668117, -1.4321692511390127,
  3.5631902724350617, 1.5373815932748647, -2.161103241376492, 1.022013270463777,
  -3.5769407392137564, -0.07778435818975193, -4.688738148618277, -3.4767694815808547,
  -2.5269340059063916, 1.3187440878267243, -2.1793236270579874, -0.2542632047916227,
  -3.120392554736342, 4.789317492267953, 3.4585286027278688, 4.274536145369288, 0.347273037908737,
  1.0391390280769013, -0.3671319555720265, -3.079761428891703, 0.5406854040986664,
  3.942124061656127, 0.19398919210419407, -3.554533398173889, 4.311199559231314,
  -0.7477180309201419, -3.745061593831609, 3.312543147306945, -3.253545945758738,
  2.971368080067041, -2.823991966134254, -1.6453365317121005, -0.23360479476551355,
  3.971661079133998, -3.6140361576165114, -4.799225609967264, 0.25836184980836396,
  -0.41374210808905243, 4.684300441844957, 2.324263122927512, -3.6332006073268475,
  3.376764427166883, 0.08161480818947453, 1.3651489773043242, -2.122053609773109,
  -0.14325667618140514, -2.8971612445275885, -3.5887415072469353, -3.4924862189958894,
  4.838803975888059, 4.416748491817469, -1.6350348437361872, -0.17428772180872354,
  -4.854662203779188, -1.9714579465230342, -4.652610687277865, 2.446506003656568,
  3.2399682919801425, -0.3441857016797867, 1.6346586314028926, 1.1525681773794316,
  1.5895075952263689, 3.1802104615836946, 3.2584889731473847, -1.9484349521056865,
  -1.4943554774310353, -4.370300733736709, 1.5639160549801545, 4.477978431087227,
  -2.1937964973545987, -0.7134165205836496, 4.052983288047843, -3.297599992189333,
  0.6963054720529449, 3.1744747376790023, 4.363169293595012, 0.49880706797350527,
  -2.650718245077066, 0.2685263298772451, 2.52885517793114, -2.144961285155098, 3.6514584199474953,
  1.7944135959884555, 0.4775690877453975, 2.1385366814701285, 0.45887075934919963,
  2.8993630000514514, -3.751469410772419, -1.3555639585801327, -3.6442895230975036,
  -4.051316493686208, 2.4696547061602185, 1.9571150346404291, -3.432577676080384,
  -3.304499753912178, 0.2571851286233713, 3.3521073690431713, 4.286280567750612,
  -3.0471527130250164, 2.7183736573342854, -3.8240684082140364, -4.30479909173672,
  -4.59319946478888, 3.535694783306111, -4.14120224730409, 4.632720024804495, -2.634118944593966,
  3.1039965408814947, -2.498089429360628, -1.480159957990892, -2.8253600851769867,
  0.07519236003353846, -2.9234474737357985, -3.6274533350769222, 4.9537645692735275,
  1.5661566978902508, -0.6345054832021901, -0.24064810013882276, -1.377172122414796,
  0.5237479880462175, -2.4877386469088836, 4.865133993816258, -4.938261819611796,
  -2.341502043009678, -2.147769869218139, -1.236921039536929, 3.9856920361015042,
  2.149141064615505, -3.293346787164806, -1.4244213363324376, -3.788486201161756,
  -1.2613070616707178, 4.313368463375223, 3.810010330360205, 3.649177339655674, 2.012650442274147,
  4.108045733436178, 2.8307385130719602, 0.3527196140657818, -1.5689419645426916,
  4.252099031741379, -2.1854484686188824, -4.837964023713061, 1.3822566684846578,
  -1.5163993909408968, -4.7909164471163805, 0.34530500246458207, 3.2515101073951787,
  3.2932574285491807, -2.577656240014765, 2.350447580214656, 3.119006519847252, -1.1467837479888598,
  -4.798003843328383, -3.084814145534971, -1.8344349919548253, 2.688724570223771,
  0.3928710512259226, 2.8209370697191494, -2.3296343982417467, -4.204863793264759,
  0.6958934750435279, -4.676646525968431, -1.1165523762342655, -1.3966641245627165,
  0.06548016785724187, -1.7234533352404302, -4.4118835159654495, -4.620030721026859,
  3.055757025983599, -3.3345807477234457, 4.06187774329439, -4.155759994078425, -4.966613116019447,
  0.2726001466461847, -1.2149442416978062, 1.1926644695531525, -3.6900021140752726,
  -0.728649113554285, 0.2334627936154714, -4.955275163377766, 3.082665550570221,
  1.1116448963494596, 1.7267572817872168, -0.2471062078672226, -3.5835159483472734,
  1.2236473091778084, 4.319101970207653, -4.860901300321454, -3.0698897109062195,
  2.655359437276541, 3.245941556740588, 0.031120256340440022, 2.360953016395615,
  2.9303085419679586, 2.2028802463737502, -1.7435779918097905, -2.7999629666034043,
  0.17905108445521556, -4.045124479038873, 3.5336318918233474, -2.5881981862352865,
  2.328173352041185, 3.9381162959856457, 0.7121024162147531, 1.0150741743265579,
  -3.818777697234619, 3.045714247146414, -0.6099037655871946, 3.394383692643416,
  -2.703496681978106, -3.1454804384246104, -4.664918048648707, 0.3901167869936062,
  -3.8024024675628154, -0.9181463757222597, 0.6278871314968963, -1.9540170319961248,
  -3.710170998571718, 1.2170504957449424, -0.9373261492062213, -2.34318768993087,
  4.353956894275182, 1.8352902194271365, 3.6067528922423513, 1.6566203198112728,
  0.23959629295691443, -4.406190945421424, 3.868647115750006, 1.5966025222779638,
  -3.9217802123139323, 3.511661787690155, 0.7408777649690883, -2.247162994868308,
  4.706359118943531, 1.6243387367010564, 3.065670279717775, -3.304224850013193, 3.0551483255846854,
  2.4081484646227747, 0.41746970848002274, -3.369111871401398, 2.798879615896846,
  -2.7048601738426936, -4.512889454490773, -3.2320784219446974, -0.0465758146900459,
  0.0038168746261346698, -4.568116712146255, -2.1374621879516376, 4.712839997077804,
  -0.029049149593758372, 1.801449255912595, 3.0294300467654587, -1.4602849210945212,
  0.6984186495753768, -3.8175429905467495, 1.4393009780300847, -4.625862003041592,
  -0.4054273348605175, -4.93813659742081, -4.68478502657713, -0.5809527978265301,
  3.7162037476787013, 3.7799612375627625, -3.137826843645384, -0.2887526529152922,
  -3.668533018106066, -2.9020947998880287, 0.5059022983598176, 3.484417649243145,
  3.1476634989061942, 1.1666912793951667, -0.31740399736955194, -0.23265771547930747,
  -3.282958091579793, -4.361360959469131, -1.2980587528594034, -4.457881136393413,
  -1.2456927298298326, -4.910501913576521, 3.062579241530498, 4.514470397226507,
  4.441624638863891, 2.0713993494012772, -1.6705392974761946, -2.155011105779745,
  3.4220418821064413, 3.6359825839977447, 2.4589165509078033, -3.840673573796154,
  3.9477363522872153, 4.104443870905799, -2.234161743777745, -3.8197175323945376,
  2.0974241815686634, -4.675184917462896, -4.509550698868237, 3.5435400829092565,
  3.3321070832224073, 1.3881107254318543, -0.07422919912869475, 3.154932322592252,
  -1.4031458516249495, 1.8355371159370115, 4.215033171038469, 3.563663503741024,
  4.541015349724436, 0.0492688188359347, -1.8389318624163309, 0.31452935952354544,
  1.7934244360391638, 3.6926209711925715, -0.10988467253906009, 1.8555737107959818,
  -3.528103870317909, 3.6003903579926266, 4.902023100151618, 0.925892251063896, -4.715654339938526,
  2.798018862190572, -0.4387036518359828, -3.188131316420928, 1.8238581307469692,
  -3.9617382429650894, -1.5008772270982873, -1.5519248169780786, 1.8771895107203163,
  -1.7240348663791738, 3.3625152231230615, -1.9888749306074982, -3.695354785751086,
  3.0385823628281763, 0.6925815208007187, 4.30473887479641, 3.6106439116684275, -0.5994042615457058,
  0.11359713100602242, -0.6900446298454543, -2.1592466346079386, 4.607083588425148,
  1.2590266645318788, -3.5724896879277646, -1.469001924478932, -4.2634768215574415,
  4.783571599034982, -4.103070505205711, 2.8265668678755107, 1.6432398032952893,
  -1.6054472854800572, -3.893934634263363, 1.01104477114641, -0.6842280375193415,
  1.5179816692836567, -0.7076672964156039, -1.2448672432262797, -1.4187391548017358,
  0.20064204315455303, -4.418434126128186, -0.3742802251704269, -4.595063390048404,
  1.377076951142067, -3.5675602512683557, -4.274963636913296, 1.2181277994249085,
  3.235440698682069, 1.9050168861627164, -2.7027805482748013, -1.5787436178931125,
  3.6491545298789276, -3.2653347841077474, 0.33157166791740966, -4.264209611520686,
  -1.897590357087795, 2.9048271099029765, -3.073306435755594, 2.899204721753936,
  2.6211276298877078, 0.38852394012506064, 0.8456467059621957, -3.607061803396563,
  0.2166833891999893, 1.1153724540994396, -2.0783861549966742, 0.3832010903273435,
  -3.131544679323186, 2.363366012480008, -2.9429711767519584, -4.591450585557397,
  -2.819046932116942, -0.5485328457623613, -2.7657228751451535, -2.373342458275254,
  3.74367791159235, 1.6044523845311742, 1.2968562169278464, -4.931023074788364, -0.9347357931743039,
  2.148897493410031, -1.9103961436219197, 0.019002521438582498, -1.4475350566948886,
  0.7434002009262102, -3.911913846748636, 1.9731500482597504, -3.653292008088993,
  3.5601297572628354, -0.28721152805444916, 3.2513564113894695, 2.781113892191822,
  -4.224057161128911, 3.379219781728766, -4.239572481440261, 4.32660003758734, 4.13434038120451,
  -3.705335677647419, -2.357662267335101, -1.5652904406691182, 2.613483474976226,
  2.077537924726527, 1.827194509358745, 2.7668844034799323, 1.6292414146107381, 1.0213267303517366,
  -2.8106271679934336, 2.4427212617926024, -4.32156849720397, -4.883844045982668,
  -3.3666619916196927, 1.456376977177289, 1.4380051581411628, -1.9658633498310119,
  -4.0239942748901925, 3.732971599102992, -2.1910613751068952, -0.860291786930258,
  2.690436742000779, 2.9402475355269253, -3.2630179644498924, 4.413033834634433,
  -1.5823739210542698, -3.714716996111358, 3.3035154782404135, -4.573435789840394,
  -1.1723199508725934, -2.8325303065845975, -2.1224628427490666, -1.4647839761372206,
  0.11608310892518237, -3.9382544004448805, 1.3541093258484853, 0.9728446085706128,
  1.78499967572664, -2.6319801351455063, -0.4496051123693867, 0.7794342706304747,
  3.0168344578531343, -4.19274891542158, 2.5628520177771623, 3.389878291455661, 3.2573816378061426,
  1.9039186327788116, 0.9559988773091854, 2.159745244110942, -2.7524911298961774,
  -2.7477375072437082, 4.484095512523021, -2.7255941119172977, -0.7444970122783996,
  -3.724344417544272, 1.9678354180495088, -1.0096788921694833, 1.9119899194004741,
  -2.6287020259678906, -4.016671677037344, -3.800807915762743, 0.8961004830778796,
  -3.1138447297605776, -3.806028700616335, -3.542805856193534, -4.67859124809487,
  0.21780450313241317, 3.703878314572604, 3.821208153683319, 2.559913390062829, 3.7315177911069526,
  -2.1638401291150733, 1.8443980448302515, 3.3947338969248086, 1.4979444422369737,
  -4.245230686263184, -3.590897838140361, -2.1652299174243206, -0.5551205701610451,
  -2.0831555689110415, 0.06329861929615, 3.6613191224698483, -0.43568592059094513,
  -2.123415615609683, 4.655867564941094, 2.504445903727504, -3.2040713968259404,
  0.8587067952435312, -4.698443712072049, 4.376256923636419, -4.276586895776577,
  4.039180306346299, -0.6844489481268408, -4.466900207467814, -4.431402174201294,
  0.3244370752849557, 0.6301392190205934, 0.5929870496341039, 2.6915918099995695,
  -3.420935405210519, -0.7315090083355518, 1.2945205326445297, -2.6971793613191553,
  1.7346515866027374, -3.899436543159003, 3.029736866217805, -4.06396296651959, -2.928717213218195,
  -1.0902639902090772, -0.8732924205036108, -2.445397019186215, -1.0826166880372057,
  2.4450667003612896, -0.7750586095797374, 2.0698778622705882, 2.2179512682903013,
  1.1323005742102836, 4.033599810095321, 3.4066255974841013, 3.446464428226861, 3.8875149774349094,
  0.8587225445839897, 4.490651509465419, 1.9132361931576902, -4.04632526265493, -3.810959202805988,
  -0.7528097432783909
]</div> <!-- .element: class="fragment fade-up-then-out" data-fragment-index="0" -->
